def launch_env() {
    sh """
        #!/bin/bash
        cp -a ${DATASET_INPUT_PATH}/. ./framework/pre-process/dataset
        make develenv-up-recreate
    """
}

def run_pipeline() {
    sh """
        #!/bin/bash
        make -C ./delivery/jenkins run-pipeline
    """
}

def stop_env() {
    sh """
        #!/bin/bash
        make develenv-down
    """
}

pipeline {
    agent { label "TORRE-UBUNTU" }
    options {
        buildDiscarder(logRotator(numToKeepStr: '10'))
    }
    parameters {
        string name: 'DATASET_INPUT_PATH', 
               defaultValue: '/home/gonzalopardo/Documents/jenkins/HAR-AI-FRAMEWORK/harvard-tunned-dataset', 
               description: 'Path to retrieve tuned data to pre-process'
        
        string name: 'SHARED_VOLUME_PATH', 
               defaultValue: '/home/gonzalopardo/Documents/jenkins/HAR-AI-FRAMEWORK/final-dataset', 
               description: 'Path to store dataset shared volume in remote node'

        gitParameter name: 'BRANCH_NAME',
                     type: 'PT_BRANCH_TAG',
                     branchFilter: 'origin/(.*)',
                     defaultValue: 'main',
                     sortMode: 'DESCENDING_SMART',
                     description: 'Select a branch to run pre-process scripts'
    }
    stages{
        stage('Run pre-process scripts') {
            agent { label "TORRE-UBUNTU" }
            steps{ 
                url: 'https://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@github.com/GonzaloPardoVillalibre/HAR-AI-FRAMEWORK.git', branch: "add-ci-integration"
                launch_env()
                run_pipeline()
                stop_env()
            }  
        }
    }
}

