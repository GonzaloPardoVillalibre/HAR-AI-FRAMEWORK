def launch_env() {
    sh """
        #!/bin/bash
        cp -a ${DATASET_INPUT_PATH}/. ./framework/pre-process/dataset
        make make train-up
    """
}

def stop_env() {
    sh """
        #!/bin/bash
        make make train-down
    """
}

pipeline {
    agent { label "TORRE-UBUNTU" }
    options {
        buildDiscarder(logRotator(numToKeepStr: '10'))
    }
    parameters {
        string name: 'DATASET_INPUT_PATH', 
               defaultValue: '/home/gonzalopardo/Documents/jenkins/HAR-AI-FRAMEWORK/harvard-tunned-dataset', 
               description: 'Path to retrieve tuned data to pre-process'
        
        string name: 'SHARED_VOLUME_PATH', 
               defaultValue: '/home/gonzalopardo/Documents/jenkins/HAR-AI-FRAMEWORK/final-dataset', 
               description: 'Path to store dataset shared volume in remote node'

        gitParameter name: 'BRANCH_NAME',
                     type: 'PT_BRANCH_TAG',
                     branchFilter: 'origin/(.*)',
                     defaultValue: 'main',
                     sortMode: 'DESCENDING_SMART',
                     description: 'Select a branch to run pre-process scripts'
    }
    stages{
        stage('Launch environment') {
            steps{ 
                launch_env()
            }  
        }
        stage('Train') {
            steps{ 
                    sh """
                        #!/bin/bash
                        make -C ./delivery/jenkins train
                    """
            }  
        }
        stage('Stop environment') {
            steps{ 
                stop_env()
            }  
        }
    }
}

