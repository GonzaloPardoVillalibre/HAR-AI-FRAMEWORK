version: "3"
services:

  #######################################
  # Processing environment
  #######################################
  preprocesser:
    container_name: preprocesser
    build:
      context: ./docker/pre-processing
    volumes:
      - ./framework/pre-processing:/TFG/framework/pre-processing
      - ./framework/final-dataset:/TFG/framework/final-dataset
    working_dir: /TFG/framework/pre-processing
    command: tail -f /dev/null

  #######################################
  # Training environment
  #######################################
  trainer:
    container_name: trainer
    build:
      context: ./docker/train
    volumes:
      - ./framework/train:/TFG/framework/train
      - ./framework/final-dataset:/TFG/framework/final-dataset
    working_dir: /TFG/framework/train
    command: tail -f /dev/null
    deploy:
      resources:
        reservations:
          devices:
          - capabilities: [gpu]

  #######################################
  # Inference API
  #######################################
  inferencer:
    container_name: inferencer
    build:
      context: ./docker/inference
    ports:
      - "8082:8082"
    volumes:
      - ./framework/inference:/TFG/framework/inference
    working_dir: /TFG/framework/inference
    command: python3 inferenceServer.py
    environment:
        LOG_LEVEL: DEBUG
        ENV: LOCAL
        SERVER_PORT: 8082
        DEFAULT_NN: N5-250-28-9-1
    deploy:
      resources:
        reservations:
          devices:
          - capabilities: [gpu]
