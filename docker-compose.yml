version: "3"
services:

  #######################################
  # Processing environment
  #######################################
  preprocesser:
    container_name: preprocesser
    build:
      context: ./delivery/docker/pre-process
    volumes:
      - ./framework/pre-process:/framework/pre-process
      - ${SHARED_VOLUME_PATH:-./framework}/final-dataset:/framework/final-dataset
    user: docker
    working_dir: /framework/pre-process
    command: tail -f /dev/null

  #######################################
  # Training environment
  #######################################
  trainer:
    container_name: trainer
    build:
      context: ./delivery/docker/train
    volumes:
      - ./framework/train:/framework/train
      - ${SHARED_VOLUME_PATH:-./framework}/final-dataset:/framework/final-dataset
    user: docker
    working_dir: /framework/train
    command: tail -f /dev/null
    # deploy:
    #   resources:
    #     reservations:
    #       devices:
    #       - capabilities: [gpu]

  #######################################
  # Inference API
  #######################################
  inferencer:
    container_name: inferencer
    build:
      context: ./delivery/docker/inference
    ports:
      - "8082:8082"
    volumes:
      - ./framework/inference:/framework/inference
    user: docker
    working_dir: /framework/inference
    command: python3 inferenceServer.py
    environment:
        LOG_LEVEL: DEBUG
        ENV: LOCAL
        SERVER_PORT: 8082
        DEFAULT_NN: N5-250-28-9-1
    # deploy:
    #   resources:
    #     reservations:
    #       devices:
    #       - capabilities: [gpu]
