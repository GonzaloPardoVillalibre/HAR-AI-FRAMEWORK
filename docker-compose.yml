version: "3"
services:

  #######################################
  # Processing environment
  #######################################
  preprocess:
    container_name: preprocess
    build:
      context: ./delivery/docker/pre-process
    volumes:
      - ./framework/pre-process:/framework/pre-process
      - ${SHARED_VOLUME_PATH:-./framework/final-dataset}:/framework/final-dataset
    user: docker
    working_dir: /framework/pre-process
    command: tail -f /dev/null

  #######################################
  # Training environment
  #######################################
  train:
    container_name: train
    build:
      context: ./delivery/docker/train
    ports:
      - "8080:6006"
    volumes:
      - ./framework/train:/framework/train
      - ${SHARED_VOLUME_PATH:-./framework/final-dataset}:/framework/final-dataset
    user: docker
    working_dir: /framework/train
    command: tail -f /dev/null
    environment:
      SHARED_VOLUME_PATH: ${SHARED_VOLUME_PATH:-./framework/final-dataset}
      OUTCOME_FOLDER_NAME: ${OUTCOME_FOLDER_NAME:-default}
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              device_ids: ['0']        
              capabilities: [gpu]

  #######################################
  # Inference API
  #######################################
  inference:
    container_name: inference
    build:
      context: ./delivery/docker/inference
    ports:
      - "8082:8082"
    volumes:
      - ./framework/inference:/framework/inference
    user: docker
    working_dir: /framework/inference
    command: python3 inferenceServer.py
    environment:
        LOG_LEVEL: DEBUG
        ENV: LOCAL
        SERVER_PORT: 8082
        DEFAULT_NN: N5-250-28-9-1
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              device_ids: ['0']        
              capabilities: [gpu]
